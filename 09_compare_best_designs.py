#!/usr/bin/env python2
# encoding: utf-8

"""\
Create a nicely organized excel spreadsheet comparing all of the validated 
designs in the given where the lowest scoring decoy within some threshold of 
the target structure.

Usage: 09_compare_best_designs.py <name> [<round>] [options]

Options:
    -t RESTRAINT_DIST --threshold RESTRAINT_DIST    [default: 1.2]
        Only consider designs where the lowest scoring decoy has a restraint 
        satisfaction distance less than the given threshold.

    -p PREFIX --prefix PREFIX
        Specify a prefix to append to all the files generated by this script.  
        This is useful for discriminating files generated by different runs.

    -v --verbose
        Output sanity checks and debugging information for each calculation.
"""

from __future__ import division

import os, re, string, itertools, numpy as np
from tools import docopt, scripting
from libraries import pipeline, structures

class Design (object):

    def __init__(self, directory):
        self.directory = directory
        self.structures = structures.load(directory)
        self.loops = pipeline.load_loops(directory)
        self.resfile = pipeline.load_resfile(directory)
        self.representative = self.rep = np.argmin(self.scores)

    def __getitem__(self, key):
        return self.structures[key]

    @property
    def scores(self):
        return self['total_score']

    @property
    def distances(self):
        return self['restraint_dist']

    @property
    def rep_path(self):
        return os.path.join(self.directory, self['path'][self.rep])

    @property
    def rep_score(self):
        return self.scores[self.rep]

    @property
    def rep_distance(self):
        return self.distances[self.rep]


class Metric (object):
    title = "Unnamed Metric"
    align = "left"
    width = 18
    color = False
    num_format = None
    font_name = None
    format = {}
    progress_update = None

    def load(self, designs, verbose=False):
        for design in designs:
            self.load_cell(design, verbose)

    def load_cell(self, design, verbose=False):
        pass

    def compare(self, design_1, design_2):
        return self.score_value(design_1) < self.score_value(design_2)

    def face_value(self, design):
        raise NotImplementedError

    def score_value(self, design):
        return self.face_value(design)

    def header_format(self):
        format = dict(bold=True, italic=True)
        if self.align is not None: format['align'] = self.align
        return format

    def cell_format(self):
        format = self.format.copy()
        if self.align is not None: format['align'] = self.align
        if self.num_format is not None: format['num_format'] = self.num_format
        if self.font_name is not None: format['font_name'] = self.font_name
        return format


class DesignNameMetric (Metric):
    title = "Design Name"
    align = 'left'
    width = 25

    def load_cell(self, design, verbose=False):
        round = re.search('round_(\d+)', design.directory).group(1)
        name = design['path'][design.rep][:-len('.pdb.gz')]
        design.name = "Round {}: {}".format(round, name)

    def face_value(self, design):
        return design.name


class ResfileSequenceMetric (Metric):
    title = "Resfile Sequence"
    align = 'left'
    font_name = 'Monospace'
    width = 30

    def load_cell(self, design, verbose=False):
        full_sequence = design['sequence'][design.rep]
        design.resfile_sequence = ''.join(
                full_sequence[i-1] for i in design.resfile.designable)

    def face_value(self, design):
        return design.resfile_sequence


class ClusterMetric (Metric):
    title = "Cluster ID"
    progress_update = "Clustering designs..."
    align = 'center'
    num_format = '0'

    def load(self, designs, verbose=False):
        for design in designs: self.read_loop_coords(design)
        self.cluster_loop_coords(designs, verbose)

    def face_value(self, design):
        return design.cluster_id

    def read_loop_coords(self, design):
        if design.rep_path.endswith('.gz'):
            from gzip import open
        else:
            from __builtin__ import open

        with open(design.rep_path) as file:
            lines = file.readlines()

        loop_coords = []
        loop_indices = []
        backbone_atoms = 'N', 'CA', 'C'

        for start, stop in design.loops:
            loop_indices += range(start, stop + 1)

        for line in lines:
            if line[0:6] != 'ATOM  ': continue

            atom_name = line[13:16].strip().upper()
            residue_id = int(line[22:26])
            atom_coord = np.array((
                    float(line[30:38]),
                    float(line[38:46]),
                    float(line[46:54])))

            if residue_id in loop_indices and atom_name in backbone_atoms:
                loop_coords.append(atom_coord)

        design.loop_coords = np.asarray(loop_coords)

    def cluster_loop_coords(self, designs, verbose=False):
        import scipy.spatial.distance as sp_dist
        import scipy.cluster.hierarchy as sp_clust
        from itertools import combinations

        num_designs = len(designs)
        if num_designs < 2: return

        # Calculate the pairwise distance matrix.

        dist_matrix = np.zeros((num_designs, num_designs))
        design_combos = itertools.combinations(enumerate(designs), 2)

        for (i, design_i), (j, design_j) in design_combos:
            dist_matrix[i,j] = self.calculate_loop_rmsd(design_i, design_j)
            dist_matrix[j,i] = dist_matrix[i,j]

        # Cluster the design such that no two designs in any cluster are 
        # further than 1.0Å apart.  This is an arbitrary cutoff, but it seems 
        # to work well.

        dist_vector = sp_dist.squareform(dist_matrix)
        hierarchy = sp_clust.complete(dist_vector)
        clusters = sp_clust.fcluster(hierarchy, 1.0, criterion='distance')

        for cluster, design in zip(clusters, designs):
            design.cluster_id = cluster

        # Print some debugging information, if requested.

        if verbose == True:
            cluster_map = {}

            for cluster, design in zip(clusters, designs):
                cluster_map.setdefault(cluster, []).append(design)

            for cluster in sorted(set(clusters)):

                # Print out the designs contained in this cluster.

                print "Cluster {}:".format(cluster)
                for design in cluster_map[cluster]:
                    print " ", design.rep_path
                print

                # Print out pairwise distances for every cluster member.

                X = cluster_map[cluster]
                N = len(X)
                D = np.zeros((N, N))

                for i, design_i in enumerate(X):
                    for j, design_j in enumerate(X):
                        D[i,j] = self.calculate_loop_rmsd(design_i, design_j)

                print sp_dist.squareform(D)
                print

                # Offer to display the cluster in pymol.

                command = ['pymol', '-qx']
                for design in cluster_map[cluster]:
                    command.append(design.rep_path)

                if raw_input("  View in pymol? [y/N] ") == 'y':
                    import subprocess
                    subprocess.check_output(command)

                print

    def calculate_loop_rmsd(self, design_1, design_2):
        assert len(design_1.loop_coords) and len(design_2.loop_coords)
        difference = design_1.loop_coords - design_2.loop_coords
        num_atoms = design_1.loop_coords.shape[0]
        return np.sqrt(np.sum(difference**2) / num_atoms)


class RestraintDistMetric (Metric):
    title = u"Restraint Dist (Å)"
    progress_update = "Calculating quality metrics..."
    align = 'center'
    num_format = '0.00'
    color = True

    def load_cell(self, design, verbose=False):
        design.restraint_dist = design.rep_distance

    def face_value(self, design):
        return design.restraint_dist

    def score_value(self, design):
        return -self.face_value(design)


class ScoreGapMetric (Metric):
    title = "Score Gap (REU)"
    align = 'center'
    num_format = '0.00'
    color = True

    def load_cell(self, design, verbose=False):
        scores = design.structures['total_score']
        distances = design.structures['restraint_dist']
        rep_score = design.structures['total_score'][design.rep]

        competitor_scores = scores.copy()
        competitor_scores[distances < 2.0] = np.inf
        competitor = np.argmin(competitor_scores)
        competitor_score = design.scores[competitor]

        design.score_gap = competitor_score - rep_score
        
    def face_value(self, design):
        return design.score_gap


class PercentSubangstromMetric (Metric):
    title = "% Subangstrom"
    align = 'center'
    num_format = '0.00'
    color = True

    def load_cell(self, design, verbose=False):
        distances = design.structures['restraint_dist']
        suba_distances = distances[distances < 1.0]
        design.percent_subangstrom = 100 * len(suba_distances) / len(distances)

    def face_value(self, design):
        return design.percent_subangstrom


class BuriedUnsatHbondMetric (Metric):
    title = "# Buried Unsats"
    align = 'center'
    num_format = '"+"0;"-"0'
    color = True

    def load_cell(self, design, verbose=False):
        design.buried_unsats = \
                design.structures['buried_unsat_score'][design.rep]

    def face_value(self, design):
        return design.buried_unsats

    def score_value(self, design):
        return -self.face_value(design)


class DunbrackScoreMetric (Metric):
    title = "Dunbrack (REU)"
    align = 'center'
    num_format = '0.00'
    color = True

    def load_cell(self, design, verbose=False):
        design.dunbrack_score = \
                design.structures['dunbrack_score'][design.rep]

    def face_value(self, design):
        return design.dunbrack_score

    def score_value(self, design):
        return -self.face_value(design)



def find_validation_workspaces(name, round=None):
    workspaces = []

    for round in [round] if round else itertools.count(1):
        workspace = pipeline.ValidatedDesigns(name, round)
        if not workspace.exists(): break
        workspaces.append(workspace)

    if not workspaces:
        scripting.print_error_and_die('No validated designs found.')

    return workspaces

def find_reasonable_designs(workspaces, threshold=None):
    print "Loading designs..."

    designs = []

    if threshold is None:
        threshold = 1.2

    for workspace in workspaces:
        for directory in workspace.output_subdirs:
            design = Design(directory)
            if design.rep_distance < float(threshold):
                designs.append(design)

    return designs

def calculate_quality_metrics(designs, verbose=False):
    # In the future, this function should look for more quality metrics in a 
    # python script that may optionally be placed in the root directory of the 
    # workspace in question.

    metrics = [cls() for cls in Metric.__subclasses__()]

    for metric in metrics:
        if metric.progress_update: print metric.progress_update
        metric.load(designs, verbose)

    return metrics

def find_pareto_optimal_designs(designs, metrics, verbose=False):
    return designs

def report_quality_metrics(designs, metrics, path):
    import xlsxwriter
    print "Reporting quality metrics..."

    # Open a XLSX worksheet.

    workbook = xlsxwriter.Workbook(path)
    worksheet = workbook.add_worksheet()

    workbook.formats[0].border = 1
    workbook.formats[0].border_color = 'gray'

    # Setup the cell background color highlights.

    from matplotlib.cm import ScalarMappable
    from matplotlib.colors import LinearSegmentedColormap
    
    best_color = np.array([114, 159, 207]) / 255
    worst_color = np.array([255, 255, 255]) / 255
    color_table = {
            'red':   [(0.0, worst_color[0], worst_color[0]),
                      (1.0,  best_color[0],  best_color[0])],
            'green': [(0.0, worst_color[1], worst_color[1]),
                      (1.0,  best_color[1],  best_color[1])],
            'blue':  [(0.0, worst_color[2], worst_color[2]),
                      (1.0,  best_color[2],  best_color[2])],
    }
    color_map = LinearSegmentedColormap('highlight', color_table)

    # Write the header row.

    for col, metric in enumerate(metrics):
        cell = string.uppercase[col] + '1'
        format = workbook.add_format(metric.header_format())
        worksheet.write(cell, metric.title, format)
        worksheet.set_column(col, col, metric.width)

    # Write the data rows.

    designs = designs[:]
    designs.sort(key=lambda x: x.restraint_dist)
    designs.sort(key=lambda x: x.buried_unsats)
    designs.sort(key=lambda x: x.cluster_id)

    for col, metric in enumerate(metrics):
        face_values = np.array([metric.face_value(x) for x in designs])
        score_values = np.array([metric.score_value(x) for x in designs])
        cell_format = metric.cell_format()

        if metric.color:
            color_spectrum = ScalarMappable(cmap=color_map)
            rgba = (255 * color_spectrum.to_rgba(score_values)).astype(int)
            colors = ['#{:02x}{:02x}{:02x}'.format(*x) for x in rgba]

        for index, face_value in enumerate(face_values):
            if metric.color:
                cell_format.update({
                    'bg_color': colors[index], 
                    'border_color': '#c7c7c7',
                    'border': 1 })

            format_handle = workbook.add_format(cell_format)
            worksheet.write(index + 1, col, face_value, format_handle)

    # Write the XLSX file.

    workbook.close()

def report_score_vs_rmsd_funnels(designs, path):
    from matplotlib.backends.backend_pdf import PdfPages
    import matplotlib.pyplot as plt

    print "Reporting score vs RMSD funnels..."

    pdf = PdfPages(path)
    designs = sorted(designs, key=lambda x: x.fancy_path)

    for index, design in enumerate(designs):
        plt.figure(figsize=(8.5, 11))
        plt.suptitle(design.fancy_path)

        axes = plt.subplot(2, 1, 1)
        plot_score_vs_dist(axes, design, metric="Max COOH Distance")

        axes = plt.subplot(2, 1, 2)
        plot_score_vs_dist(axes, design, metric="Loop RMSD")

        pdf.savefig(orientation='portrait')
        plt.close()

    pdf.close()

def report_pymol_sessions(designs, directory):
    print "Reporting pymol sessions..."

    if os.path.exists(directory): shutil.rmtree(directory)
    os.mkdir(directory)

    with open('pymol_modes.txt') as file:
        import yaml
        base_config = yaml.load_cell(file)['Present design in pymol']

    for design in designs:
        decoy = design.representative
        config = base_config + ' save ' + os.path.join(
                directory, design.get_fancy_path('.pse')) + ';'

        score_vs_distance.open_in_pymol(design, decoy, config, gui=False)


with scripting.catch_and_print_errors():
    args = docopt.docopt(__doc__)
    prefix = args['--prefix'] or ''

    workspaces = find_validation_workspaces(args['<name>'], args['<round>'])
    designs = find_reasonable_designs(workspaces, args['--threshold'])
    metrics = calculate_quality_metrics(designs, args['--verbose'])
    designs = find_pareto_optimal_designs(designs, metrics, args['--verbose'])

    report_quality_metrics(designs, metrics, prefix + 'quality_metrics.xlsx')
    #report_score_vs_rmsd_funnels(designs, '{}score_vs_rmsd.pdf'.format(prefix))
    #report_pymol_sessions(designs, '{}pymol_sessions'.format(prefix))

